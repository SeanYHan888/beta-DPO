# ========================================
# Stage 1: SFT (Supervised Fine-Tuning)
# ========================================
python -u train.py \
    model=llama32-1b \
    datasets=[hh] \
    loss=sft \
    exp_name=llama32_1b_sft \
    batch_size=16 \
    eval_batch_size=8 \
    gradient_accumulation_steps=4 \
    trainer=BasicTrainer \
    sample_during_eval=false

# ========================================
# Stage 2: DPO (Direct Preference Optimization)
# NOTE: Update model.archive path after SFT completes
# ========================================
python -u train.py \
    model=llama32-1b \
    datasets=[hh] \
    loss=dpo \
    loss.beta=0.1 \
    exp_name=llama32_1b_dpo \
    batch_size=8 \
    eval_batch_size=4 \
    gradient_accumulation_steps=8 \
    trainer=BasicTrainer \
    sample_during_eval=false \
    model.archive=/path/to/llama32_1b_sft_TIMESTAMP/LATEST/policy.pt
